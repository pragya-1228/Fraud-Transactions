# ğŸ’³ Fraud Detection for Blocker Fraud Company

This project is a Data Science & Machine Learning internship solution for the **Blocker Fraud Company**, which specializes in detecting fraudulent mobile financial transactions. The model was designed to maximize business value using high-performance machine learning techniques and risk-based cost modeling.

---

## ğŸ“Œ Business Problem

Blocker Fraud's business model is performance-based:
- âœ… **25% of the value** of each correctly identified fraud.
- âš ï¸ **5% of the value** of each legitimate transaction incorrectly flagged as fraud.
- âŒ **100% reimbursement** for each fraudulent transaction incorrectly marked as legitimate.

Due to this high-risk, high-reward model, the **accuracy and precision** of the model directly impact the companyâ€™s profitability.

---

## ğŸ¯ Objective

Develop a predictive system that:
- Detects fraudulent transactions with high precision and recall.
- Minimizes financial losses due to false negatives.
- Simulates business revenue and loss scenarios.

---

## ğŸ“‚ Dataset Overview

- ğŸ“¦ File: `Fraud.csv`
- ğŸ“Š Shape: 6,362,620 rows Ã— 10 columns
- ğŸ¯ Target: `isFraud` (1 = fraud, 0 = not fraud)

### âœï¸ Columns Summary
| Column Name        | Description                              |
|--------------------|------------------------------------------|
| `step`             | Hour-wise time unit                      |
| `type`             | Transaction type                         |
| `amount`           | Amount of transaction                    |
| `oldbalanceOrg`    | Sender's balance before transaction      |
| `newbalanceOrig`   | Sender's balance after transaction       |
| `oldbalanceDest`   | Receiver's balance before transaction    |
| `newbalanceDest`   | Receiver's balance after transaction     |
| `isFraud`          | Whether transaction was fraud            |
| `isFlaggedFraud`   | Internally flagged fraud (rule-based)    |

---

## ğŸ§ª Methodology

### 1. Data Description
- Checked and handled missing values
- Analyzed statistical metrics: mean, median, skewness, kurtosis, etc.

### 2. Feature Engineering
- Generated hypotheses using mind mapping
- Created new derived features to improve prediction

### 3. Data Filtering
- Removed non-informative and unrealistic records (e.g., out-of-range ages)

### 4. Exploratory Data Analysis
- Performed univariate, bivariate, and multivariate analysis
- Validated hypotheses:
  - âœ… All frauds are above â‚¹10,000
  - âŒ Fraud does **not** occur 60% of the time in CASH_OUT
  - âŒ High-value transactions are not limited to TRANSFER types

### 5. Data Preparation
- Applied label encoding
- Scaled features
- Handled class imbalance

### 6. Feature Selection
- Used the **Boruta algorithm** to select relevant features
- Reduced overfitting and improved interpretability

### 7. Model Building
- Trained various models with cross-validation:
  - Logistic Regression
  - K-Nearest Neighbors
  - Random Forest
  - **XGBoost** (Best Performing)
  - LightGBM
  - Dummy Classifier (baseline)

### 8. Hyperparameter Tuning
- Applied fine-tuning on the best model (XGBoost) for optimal performance

### 9. Conclusion
- Evaluated the generalization capacity on unseen test data

### 10. Model Deployment
- Model and functions saved
- **Flask API** created for production use (deployment plan on Heroku)

---

## ğŸ“Š Cross-Validation Results

| Model                  | Balanced Accuracy | Precision        | Recall           | F1 Score         | Kappa           |
|------------------------|-------------------|------------------|------------------|------------------|-----------------|
| Dummy                  | 0.500 Â± 0.000      | 0.000 Â± 0.000     | 0.000 Â± 0.000     | 0.000 Â± 0.000     | 0.000 Â± 0.000    |
| Logistic Regression    | 0.654 Â± 0.002      | 0.953 Â± 0.007     | 0.309 Â± 0.005     | 0.466 Â± 0.005     | 0.466 Â± 0.005    |
| K Nearest Neighbors    | 0.801 Â± 0.006      | 0.949 Â± 0.005     | 0.602 Â± 0.011     | 0.736 Â± 0.008     | 0.736 Â± 0.008    |
| Support Vector Machine | 0.595 Â± 0.013      | 1.000 Â± 0.000     | 0.190 Â± 0.026     | 0.319 Â± 0.037     | 0.319 Â± 0.037    |
| Random Forest          | 0.896 Â± 0.006      | 0.970 Â± 0.003     | 0.793 Â± 0.011     | 0.872 Â± 0.007     | 0.872 Â± 0.007    |
| XGBoost                | 0.919 Â± 0.006      | 0.955 Â± 0.006     | 0.839 Â± 0.012     | 0.893 Â± 0.009     | 0.893 Â± 0.009    |
| LightGBM               | 0.698 Â± 0.123      | 0.329 Â± 0.260     | 0.404 Â± 0.238     | 0.345 Â± 0.265     | 0.344 Â± 0.266    |



## ğŸ† Best Model: XGBoost

The **XGBoost classifier** was the best-performing model based on cross-validation and test set performance. After hyperparameter tuning, it achieved strong metrics on both training and unseen data.

### ğŸ” Cross-Validation Performance

| Metric             | Score (Â± Std Dev)    |
|--------------------|----------------------|
| Balanced Accuracy  | 0.919 Â± 0.006        |
| Precision          | 0.955 Â± 0.006        |
| Recall             | 0.839 Â± 0.012        |
| F1 Score           | 0.893 Â± 0.009        |
| Cohenâ€™s Kappa      | 0.893 Â± 0.009        |

---

### ğŸ§ª Test Set (Unseen Data) Performance

| Metric             | Score                |
|--------------------|----------------------|
| Balanced Accuracy  | 0.915                |
| Precision          | 0.944                |
| Recall             | 0.829                |
| F1 Score           | 0.883                |
| Cohenâ€™s Kappa      | 0.883                |

---

### âœ… Highlights
- Successfully captured **~83% of fraudulent transactions** in unseen data.
- Achieved **high precision** (~95%) reducing the rate of false positives.
- Balanced accuracy shows robustness against extreme class imbalance.




---

## ğŸ“Š Business Simulation Results

| Scenario                                | Amount (R$)         |
|-----------------------------------------|----------------------|
| âœ… Revenue from correct fraud detection  | 60,613,782.88        |
| âš ï¸ Revenue from false positives           | 183,866.98           |
| âŒ Refund due to false negatives         | -3,546,075.42        |
| ğŸ’° **Net Profit using model**            | **57,251,574.44**    |
| âŒ Profit using existing method          | **-246,001,206.94**  |

---

## ğŸ“š Lessons Learned
- Models can be accurate even with extreme class imbalance (<1% fraud)
- Hypothesis testing strengthens feature engineering
- Business cost modeling is vital in high-risk industries like fraud detection

---

## ğŸ› ï¸ Tech Stack

- Python 3.8+
- Pandas, NumPy, Seaborn, Matplotlib
- Scikit-learn, XGBoost, LightGBM, BorutaPy
- Flask (for deployment)
- Jupyter Notebook

---

## ğŸ§ª How to Run

```bash
# 1. Clone the repository
git clone https://github.com/yourusername/blocker-fraud-detection.git
cd blocker-fraud-detection

# 2. Install dependencies
pip install -r requirements.txt

# 3. Run the Jupyter Notebook
jupyter notebook

# 4. (Optional) Run the Flask API
python app.py
